{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imports\n",
    "from recipe_scrapers import scrape_me, scrape_html\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape a single recipe in a given website\n",
    "\n",
    "Use the recipe_scrapers pypi package to scrape the website and get the title, ingredients, and instructions of the recipe. Only add them to the table if neither of those values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a single recipe from the recipe URL\n",
    "def read_recipe(page_url, a):\n",
    "    \"\"\"\n",
    "    Read a single recipe from the recipe URL.\n",
    "    Args:\n",
    "        page_url (str): The URL of the page containing the recipe.\n",
    "        a (BeautifulSoup object): The anchor tag containing the recipe link.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the recipe title, ingredients, and instructions if any, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        href = a['href']\n",
    "        recipe_url = urljoin(page_url, href)\n",
    "\n",
    "        # Scrape the actual recipe\n",
    "        scraped = scrape_me(recipe_url)\n",
    "\n",
    "        # If there is no title, return None\n",
    "        if scraped.title() != None and scraped.title() != 'None' and scraped.title() != '':    \n",
    "\n",
    "            try:\n",
    "                recipe = {'Title': scraped.title(), 'Ingredients': scraped.ingredients(), 'Instructions': scraped.instructions(), 'URL': recipe_url}\n",
    "                return recipe\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping recipe {recipe_url}: {e}\")\n",
    "                return None\n",
    "            \n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping recipe {recipe_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find recipes on the page\n",
    "\n",
    "If there are any HTML elements with either the 'recipe-title' or 'page' class, then we return those attributes, else, we find all the links on the page. This is because not all websites use the same format of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any recipes on the page URL\n",
    "def check_if_recipes_on_page(page_url, page_soup):\n",
    "    \"\"\"\n",
    "    Check if there are any recipes on the page URL by either\n",
    "    1. Looking for the class 'recipe-title' in the anchor tags, or,\n",
    "    2. Looking for the class 'page' in the anchor tags, or,\n",
    "    3. Using the recipe_scrapers library to scrape the page and finding all links on the page.\n",
    "    Args:\n",
    "        page_url (str): The URL of the page to check.\n",
    "        page_soup (BeautifulSoup object): The BeautifulSoup object of the page.\n",
    "    Returns:\n",
    "        list: A list of anchor tags containing the recipes on the page or all links on the page if any, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        recipes_on_page = page_soup.findAll('a', {'class': lambda x: x and 'recipe-title' in x.split()})\n",
    "\n",
    "        if len(recipes_on_page) == 0:\n",
    "            recipes_on_page = page_soup.findAll('a', {'class': lambda x: x and 'page' in x.split()})\n",
    "\n",
    "            if len(recipes_on_page) == 0:\n",
    "                recipes_on_page = scrape_me(page_url).links()\n",
    "\n",
    "        return recipes_on_page\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking recipes on page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the recipes on a page\n",
    "\n",
    "If we found any recipes on the page, then we go through each attribute/link and scrape the recipes from the page by entering each recipe link individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the recipes on the paritcular page URL\n",
    "def read_recipes_on_page(page_url):\n",
    "    \"\"\"\n",
    "    Read all the recipes on the page URL.\n",
    "    Args:\n",
    "        page_url (str): The URL of the page to read recipes from.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the recipes on the page if any, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_response = requests.get(page_url)\n",
    "        page_soup = BeautifulSoup(page_response.text, \"html.parser\")\n",
    "\n",
    "        recipes = []\n",
    "        \n",
    "        if page_soup:\n",
    "        \n",
    "            recipes_on_page = check_if_recipes_on_page(page_url, page_soup)\n",
    "            \n",
    "            for a in tqdm(recipes_on_page):\n",
    "                recipe = read_recipe(page_url, a)\n",
    "\n",
    "                if recipe != None:\n",
    "                    recipes.append(recipe)\n",
    "\n",
    "                    # To avoid server timeouts\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            return recipes\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading recipes on page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the next page + Going to the next page\n",
    "\n",
    "A lot of website have have the 1,2,3.., Next, Last page structure for the recipes. We try to find the HTML elements that either state \"page\" or \"next\" in them and return those references.\n",
    "\n",
    "If we are on the first page and we want to go to the second, we have to find which URL corresponds to the second page among the returned references from the previous part.\n",
    "\n",
    "Among the references we find the page numbers and if it matches the page number of the next page that we keep track of, then we return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to the next page in a website\n",
    "def go_to_next_page(recipes_url, next_page):\n",
    "    \"\"\"\n",
    "    Go to the next page in a website.\n",
    "    Args:\n",
    "        recipes_url (str): The base URL of the recipes website.\n",
    "        next_page (int): The next page number to find.\n",
    "    Returns:\n",
    "        str: The URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    recipe_page_response = requests.get(recipes_url)\n",
    "    recipe_page_soup = BeautifulSoup(recipe_page_response.text, \"html.parser\")\n",
    "    page_html = str(recipe_page_soup.prettify()).split('<')\n",
    "\n",
    "    # Check if any of the references contain the string 'page' or 'next'\n",
    "    page_refs = [val for val in page_html if re.search(r\"(([Pp][Aa][Gg][Ee]).?\\d+)\", val)]\n",
    "\n",
    "    if len(page_refs) == 0:\n",
    "        page_refs = [val for val in page_html if re.search(r\"(([Nn][Ee][Xx][Tt]).?\\d+)\", val)]\n",
    "    \n",
    "    next_page_url = find_next_page(page_refs, next_page, recipes_url)\n",
    "\n",
    "    if next_page_url:\n",
    "        return next_page_url\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the next page in a website\n",
    "def find_next_page(page_refs, next_page, recipes_url):\n",
    "    \"\"\"\n",
    "    Find the next page in a website.\n",
    "    Args:\n",
    "        page_refs (list): A list of anchor tags containing the next page links.\n",
    "        next_page (int): The next page number to find.\n",
    "        recipes_url (str): The base URL of the recipes website.\n",
    "    Returns:\n",
    "        str: The URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    if len(page_refs) > 0:\n",
    "\n",
    "        for i, page_ref in enumerate(page_refs):\n",
    "\n",
    "            # Check whether the string contain a 'href' tag\n",
    "            href = re.search(r'(href=\"[^\"]*\")', page_ref)\n",
    "            # Extract only the href tag from the string\n",
    "            clean_href = href.group(0).replace('href=\"', '').replace('\"', '')\n",
    "\n",
    "            if re.search(r'(\\d+)', clean_href).group(0) == str(next_page):\n",
    "                page_url = urljoin(recipes_url, clean_href.split('/recipes/')[-1])                \n",
    "                return page_url\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all recipes on a website\n",
    "\n",
    "Using all the functions above, we can go through multiple pages within one website and scrape all the recipes over the multiple pages!\n",
    "\n",
    "The websites might be structured differently. Often the recipes are in a location similar to \"BASE_URL/recipes/\" so we try access this first and if there a valid response from the webpage, then we continue to use this as our base URL. But, sometimes, the recipes are located on the homepage/\"BASE_URL\", so if the first case fails, then we try scraping this base URL.\n",
    "\n",
    "Here we also keep track of what page we are on and stop after scraping page 100. This is just a safety check to avoid any unforeseen infinite while loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the recipes from the website\n",
    "def read_all_recipes_on_url(website_url):\n",
    "    \"\"\"\n",
    "    Read all the recipes from the website.\n",
    "    Args:\n",
    "        recipes_url (str): The base URL of the recipes website.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing all the recipes on the website.\n",
    "    \"\"\"\n",
    "    curr_page = 1   # Set current page and increment it by 1 for each page\n",
    "    last_page = False\n",
    "\n",
    "    # Some URLs end with a '/' and some do not, so we need to remove it; it is added when joining with '/recipes/'\n",
    "    if website_url[-1] == '/':\n",
    "        website_url = website_url[:-1]\n",
    "    \n",
    "    try:\n",
    "        recipes_url = website_url+'/recipes/'\n",
    "        page_response = requests.get(recipes_url) # Throws an error if the page does not exist\n",
    "        if page_response.status_code != 200:\n",
    "            raise Exception(f\"Page not found: {recipes_url}\")\n",
    "    except:\n",
    "        print(f\"Error accessing {recipes_url}. Trying the base URL.\")\n",
    "        recipes_url = website_url\n",
    "\n",
    "    all_recipes = []\n",
    "\n",
    "    # While there are still pages to read\n",
    "    while not last_page:\n",
    "\n",
    "        try:\n",
    "            # If the current page is 1, use the base URL, otherwise go to the next page\n",
    "            if curr_page == 1:\n",
    "                page_url = recipes_url\n",
    "            else:\n",
    "                page_url = go_to_next_page(recipes_url, curr_page)\n",
    "            \n",
    "            print(f'Page {curr_page}: {page_url}')\n",
    "            recipes = read_recipes_on_page(page_url)\n",
    "\n",
    "            # If there there are recipes on the page, increment the page number, else break the loop\n",
    "            if recipes:\n",
    "                all_recipes.extend(recipes)\n",
    "                curr_page += 1\n",
    "            else:\n",
    "                last_page = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading recipes on page {curr_page}: {e}\")\n",
    "            last_page = True\n",
    "\n",
    "        # IMPORTANT: Set a maximum page limit to avoid infinite loops!!!!\n",
    "        if curr_page == 100:\n",
    "            print(\"Reached maximum page limit.\")\n",
    "            last_page = True\n",
    "            \n",
    "    return all_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get multiple websites to be scraped\n",
    "\n",
    "The recipe scraper is compatible with certain websites already and they are all listed on the documentation website. To avoid copying and pasting over 150 websites, we decided to also scrape the documentation.\n",
    "\n",
    "While scraping through all the links on the page, we try our best to only scrape recipe websites and also only ones that are in English by looking for specific extensions such as \".com/\" or \".co.uk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all website URLs from scraper pypi source documentation\n",
    "def get_all_website_urls():\n",
    "    \"\"\"\n",
    "    Get all website URLs from the scraper pypi source documentation at \"https://pypi.org/project/recipe-scrapers-ap-fork/\".\n",
    "    Returns:\n",
    "        list: A list of website URLs.\n",
    "    \"\"\"\n",
    "    page_response = requests.get(\"https://pypi.org/project/recipe-scrapers-ap-fork/\")\n",
    "    page_soup = BeautifulSoup(page_response.text, \"html.parser\")\n",
    "    page_html = str(page_soup.prettify()).split('<')\n",
    "\n",
    "    websites = []\n",
    "\n",
    "    for item in page_html:\n",
    "\n",
    "        # Checks to make sure we only extract cooking websites\n",
    "        # rel=\"nofollow\" is used to check if the link is a recipe website link\n",
    "        if 'rel=\"nofollow\"' in item and 'https' in item and not any(val in item for val in ['pypi', 'github', 'pepy', 'python', 'project']):\n",
    "            # Check whether website is in english\n",
    "            if '.com/' in item or '.co.uk/' in item:\n",
    "                # Manual addition of website since the url does not properly extract the website\n",
    "                if 'justonecookbook' in item:\n",
    "                    websites.append('https://www.justonecookbook.com/')\n",
    "                else:\n",
    "                    href = re.search(r'(href=\"[^\"]*\")', item)\n",
    "                    clean_href = href.group(0).replace('href=\"', '').replace('\"', '')\n",
    "                    websites.append(clean_href)\n",
    "    \n",
    "    return websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function to scrape all recipes from all websites\n",
    "\n",
    "We then put together all the functions to scrape the multiple viable websites found! We also keep track how many websites and recipes we have scraped for reporting purposes.\n",
    "\n",
    "After all websites have been scraped, the recipes are turned into a dataframe and any duplicates are dropped. We also make sure that all columns of the dataframe are populated, else, we remove them.\n",
    "\n",
    "The final dataset is then the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main function to run the scraper\n",
    "def main():\n",
    "    \n",
    "    # Number of websites scraped\n",
    "    website_count = 0\n",
    "\n",
    "    # Get all website URLs\n",
    "    websites = get_all_website_urls()\n",
    "    # websites = ['https://www.archanaskitchen.com/']   # test website\n",
    "\n",
    "    # Store all recipes\n",
    "    all_recipes = []\n",
    "\n",
    "    # Make sure there are websites to scrape\n",
    "    if websites:\n",
    "\n",
    "        # Loop through each website and scrape the recipes\n",
    "        for website in websites:\n",
    "\n",
    "            print(f\"Scraping recipes from {website}...\")\n",
    "            recipes = read_all_recipes_on_url(website)\n",
    "\n",
    "            if recipes:\n",
    "                all_recipes.extend(recipes)\n",
    "                print(f\"Scraped {len(recipes)} recipes from {website}.\")\n",
    "                website_count += 1\n",
    "            else:\n",
    "                print(f\"No recipes found on {website}.\")\n",
    "    \n",
    "    if len(all_recipes) == 0:\n",
    "        print(\"No recipes found in any of the websites!!\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Scraped {len(all_recipes)} recipes from {website_count} different websites.\")\n",
    "        # Make a dataframe from the recipes\n",
    "        df = pd.DataFrame.from_records(all_recipes)\n",
    "        # Drop duplicate recipes based on the title\n",
    "        df_sub = df.drop_duplicates(subset=['Title'], keep='first')\n",
    "        # Make sure all columns are populated for a recipe\n",
    "        df_clean = df_sub[(df_sub['Title']!='None')&(len(df_sub['Ingredients'])!=0)&(df_sub['Instructions']!='')].reset_index(drop=True)\n",
    "\n",
    "        # Convert df to csv file\n",
    "        if not os.path.exists('recipes'):\n",
    "            os.makedirs('recipes')\n",
    "        df_clean.to_csv('recipes/recipes.csv', index=False)\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get only english recipes\n",
    "\n",
    "Turns out, even with our checks when getting the website links to only get recipes in english, there were some other languages that managed to sneak their way in. In order to have our recipe scraper outputs be consistent, we decided it be best to simply be rid of the non-english recipes for the time being. \n",
    "\n",
    "We do this by using an open source Python package called \"langdetect\". It is used to categorize whether the instructions of a recipe/row are in english or not, and then only the recipes in english are kept and saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df = pd.read_csv(r'recipes\\recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy Salad Recipes: Chopped Salad &amp; More</td>\n",
       "      <td>['1 recipe Homemade Italian Dressing', '1 Roma...</td>\n",
       "      <td>Make the Homemade Italian Dressing.\\nChop roma...</td>\n",
       "      <td>https://acouplecooks.com/best-salad-recipes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 Easy Salad Dressing Recipes</td>\n",
       "      <td>['2 tablespoons aged balsamic vinegar', '2 tab...</td>\n",
       "      <td>For the balsamic vinaigrette\\nIn a medium bowl...</td>\n",
       "      <td>https://acouplecooks.com/salad-dressing-recipes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21 Green Salad Recipes</td>\n",
       "      <td>['1 recipe Homemade Italian Dressing', '1 Roma...</td>\n",
       "      <td>Make the Homemade Italian Dressing.\\nChop roma...</td>\n",
       "      <td>https://acouplecooks.com/green-salad-recipes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Winter Salad with Pear &amp; Greens</td>\n",
       "      <td>['1 head radicchio, washed, dried and torn int...</td>\n",
       "      <td>Make the dressing: In a medium bowl, whisk tog...</td>\n",
       "      <td>https://acouplecooks.com/winter-salad-recipes/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15 Easy Vegan Salads</td>\n",
       "      <td>['1/2 recipe Homemade Croutons', '1 romaine he...</td>\n",
       "      <td>If using, make the Homemade Croutons.\\nWash an...</td>\n",
       "      <td>https://acouplecooks.com/vegan-salad-recipes/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0  Easy Salad Recipes: Chopped Salad & More   \n",
       "1            30 Easy Salad Dressing Recipes   \n",
       "2                    21 Green Salad Recipes   \n",
       "3           Winter Salad with Pear & Greens   \n",
       "4                      15 Easy Vegan Salads   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  ['1 recipe Homemade Italian Dressing', '1 Roma...   \n",
       "1  ['2 tablespoons aged balsamic vinegar', '2 tab...   \n",
       "2  ['1 recipe Homemade Italian Dressing', '1 Roma...   \n",
       "3  ['1 head radicchio, washed, dried and torn int...   \n",
       "4  ['1/2 recipe Homemade Croutons', '1 romaine he...   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  Make the Homemade Italian Dressing.\\nChop roma...   \n",
       "1  For the balsamic vinaigrette\\nIn a medium bowl...   \n",
       "2  Make the Homemade Italian Dressing.\\nChop roma...   \n",
       "3  Make the dressing: In a medium bowl, whisk tog...   \n",
       "4  If using, make the Homemade Croutons.\\nWash an...   \n",
       "\n",
       "                                                URL  \n",
       "0      https://acouplecooks.com/best-salad-recipes/  \n",
       "1  https://acouplecooks.com/salad-dressing-recipes/  \n",
       "2     https://acouplecooks.com/green-salad-recipes/  \n",
       "3    https://acouplecooks.com/winter-salad-recipes/  \n",
       "4     https://acouplecooks.com/vegan-salad-recipes/  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['Language'] = recipes_df['Instructions'].apply(lambda x: detect(x) if isinstance(x, str) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>URL</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy Salad Recipes: Chopped Salad &amp; More</td>\n",
       "      <td>['1 recipe Homemade Italian Dressing', '1 Roma...</td>\n",
       "      <td>Make the Homemade Italian Dressing.\\nChop roma...</td>\n",
       "      <td>https://acouplecooks.com/best-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 Easy Salad Dressing Recipes</td>\n",
       "      <td>['2 tablespoons aged balsamic vinegar', '2 tab...</td>\n",
       "      <td>For the balsamic vinaigrette\\nIn a medium bowl...</td>\n",
       "      <td>https://acouplecooks.com/salad-dressing-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21 Green Salad Recipes</td>\n",
       "      <td>['1 recipe Homemade Italian Dressing', '1 Roma...</td>\n",
       "      <td>Make the Homemade Italian Dressing.\\nChop roma...</td>\n",
       "      <td>https://acouplecooks.com/green-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Winter Salad with Pear &amp; Greens</td>\n",
       "      <td>['1 head radicchio, washed, dried and torn int...</td>\n",
       "      <td>Make the dressing: In a medium bowl, whisk tog...</td>\n",
       "      <td>https://acouplecooks.com/winter-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15 Easy Vegan Salads</td>\n",
       "      <td>['1/2 recipe Homemade Croutons', '1 romaine he...</td>\n",
       "      <td>If using, make the Homemade Croutons.\\nWash an...</td>\n",
       "      <td>https://acouplecooks.com/vegan-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0  Easy Salad Recipes: Chopped Salad & More   \n",
       "1            30 Easy Salad Dressing Recipes   \n",
       "2                    21 Green Salad Recipes   \n",
       "3           Winter Salad with Pear & Greens   \n",
       "4                      15 Easy Vegan Salads   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  ['1 recipe Homemade Italian Dressing', '1 Roma...   \n",
       "1  ['2 tablespoons aged balsamic vinegar', '2 tab...   \n",
       "2  ['1 recipe Homemade Italian Dressing', '1 Roma...   \n",
       "3  ['1 head radicchio, washed, dried and torn int...   \n",
       "4  ['1/2 recipe Homemade Croutons', '1 romaine he...   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  Make the Homemade Italian Dressing.\\nChop roma...   \n",
       "1  For the balsamic vinaigrette\\nIn a medium bowl...   \n",
       "2  Make the Homemade Italian Dressing.\\nChop roma...   \n",
       "3  Make the dressing: In a medium bowl, whisk tog...   \n",
       "4  If using, make the Homemade Croutons.\\nWash an...   \n",
       "\n",
       "                                                URL Language  \n",
       "0      https://acouplecooks.com/best-salad-recipes/       en  \n",
       "1  https://acouplecooks.com/salad-dressing-recipes/       en  \n",
       "2     https://acouplecooks.com/green-salad-recipes/       en  \n",
       "3    https://acouplecooks.com/winter-salad-recipes/       en  \n",
       "4     https://acouplecooks.com/vegan-salad-recipes/       en  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>3554</td>\n",
       "      <td>3554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh-cn</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           URL  Language\n",
       "Language                \n",
       "en        3554      3554\n",
       "fr         114       114\n",
       "ja          16        16\n",
       "pl          48        48\n",
       "pt          52        52\n",
       "tr           3         3\n",
       "zh-cn        2         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df[['URL', 'Language']].groupby(recipes_df['Language']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>URL</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy Salad Recipes: Chopped Salad &amp; More</td>\n",
       "      <td>['1 recipe Homemade Italian Dressing', '1 Roma...</td>\n",
       "      <td>Make the Homemade Italian Dressing.\\nChop roma...</td>\n",
       "      <td>https://acouplecooks.com/best-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 Easy Salad Dressing Recipes</td>\n",
       "      <td>['2 tablespoons aged balsamic vinegar', '2 tab...</td>\n",
       "      <td>For the balsamic vinaigrette\\nIn a medium bowl...</td>\n",
       "      <td>https://acouplecooks.com/salad-dressing-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21 Green Salad Recipes</td>\n",
       "      <td>['1 recipe Homemade Italian Dressing', '1 Roma...</td>\n",
       "      <td>Make the Homemade Italian Dressing.\\nChop roma...</td>\n",
       "      <td>https://acouplecooks.com/green-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Winter Salad with Pear &amp; Greens</td>\n",
       "      <td>['1 head radicchio, washed, dried and torn int...</td>\n",
       "      <td>Make the dressing: In a medium bowl, whisk tog...</td>\n",
       "      <td>https://acouplecooks.com/winter-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15 Easy Vegan Salads</td>\n",
       "      <td>['1/2 recipe Homemade Croutons', '1 romaine he...</td>\n",
       "      <td>If using, make the Homemade Croutons.\\nWash an...</td>\n",
       "      <td>https://acouplecooks.com/vegan-salad-recipes/</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0  Easy Salad Recipes: Chopped Salad & More   \n",
       "1            30 Easy Salad Dressing Recipes   \n",
       "2                    21 Green Salad Recipes   \n",
       "3           Winter Salad with Pear & Greens   \n",
       "4                      15 Easy Vegan Salads   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  ['1 recipe Homemade Italian Dressing', '1 Roma...   \n",
       "1  ['2 tablespoons aged balsamic vinegar', '2 tab...   \n",
       "2  ['1 recipe Homemade Italian Dressing', '1 Roma...   \n",
       "3  ['1 head radicchio, washed, dried and torn int...   \n",
       "4  ['1/2 recipe Homemade Croutons', '1 romaine he...   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  Make the Homemade Italian Dressing.\\nChop roma...   \n",
       "1  For the balsamic vinaigrette\\nIn a medium bowl...   \n",
       "2  Make the Homemade Italian Dressing.\\nChop roma...   \n",
       "3  Make the dressing: In a medium bowl, whisk tog...   \n",
       "4  If using, make the Homemade Croutons.\\nWash an...   \n",
       "\n",
       "                                                URL Language  \n",
       "0      https://acouplecooks.com/best-salad-recipes/       en  \n",
       "1  https://acouplecooks.com/salad-dressing-recipes/       en  \n",
       "2     https://acouplecooks.com/green-salad-recipes/       en  \n",
       "3    https://acouplecooks.com/winter-salad-recipes/       en  \n",
       "4     https://acouplecooks.com/vegan-salad-recipes/       en  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_recipes = recipes_df[recipes_df['Language'] == 'en'].reset_index(drop=True)\n",
    "english_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_recipes.to_csv(r'recipes\\english_recipes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
