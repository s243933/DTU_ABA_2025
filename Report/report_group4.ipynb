{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42578 - Advanced business analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members:\n",
    " - Fie Christina MÃ¸ller: s205291\n",
    " - Raquel Moleiro Marques: s243636\n",
    " - Sabina Maria Kozlowska: s233185\n",
    " - Sree Keerthi Desu: s243933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Scrapper](#scrapper)\n",
    "- [Data exploration](#data-exploration)\n",
    "- [LLM](#llm)\n",
    "- [App](#app)\n",
    "- [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food waste is a pressing global challange, with tons of food discarded each year. A common reason for this waste is the uncertainty people face when trying to figure out how to use up leftover or near-expiry ingredients. This project, the Anti-Food Recommender, addressses this issue by suggesting recipes for using up near-expiry ingredients. \n",
    "\n",
    "To build this system, a web scraper was developed to automatically collects thousands of real-world recipes from supported cooking websites. Each recipe its title, ingredients, instructions and source URL. This dataset forms the backbone of the solution. \n",
    "\n",
    "A Large Language Model (LLM) from Hugging Face was build to derive structured insights - such as cooking methods and cuisine categories - from raw recipe text. Through a dedicated script, the LLM analyzes the each recipe to determine which ingredients undergo heat processing (bioling, baking, frying, etc.) - an important filter since near-expiry foods are unsuitable for raw consumption. It also classifies each recipe into three relevant categories, enriching the dataset with additional context useful for users and potential personalization. \n",
    "\n",
    "Finally, a user-friendly web app was developed where users can enter a list of near-expired ingredients and specify dietary preferences (vegan, vegetarian). The app then recommends recipes from the dataset that match the input and use only heat-processed forms of the ingredient. This ensures that users receive suggestions that are not only relevant but also make the most out of the near-expiry ingredients by turning them into enjoyable meals.\n",
    "\n",
    "By combining automated data collection, advanced AI analysis, and a responsive front-end, the Anti-Food Waste Recommender offers a scalable tool to help reduce food waste. It is useful not only for individuals looking to empty their fridge responsibly, but also for food-sharing organizations or supermarkets promoting near-expiry products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support the development of the Anti-Food Waste Recommender, a dedicated web scraper was designed to collect large-scale, structured recipe data from online cooking platforms. These recipes serve as foundation of the recommendation engine, enabling it to suggest realistic ways to use up ingredients that are nearing expiration. The scraper automatically gathers thousands of authentic recipes from cooking websites. \n",
    "\n",
    "In this report, the scraping logic is demonstrated within the Jypiter Notebook to allow step-by-step explanation. The implementation is based on a single Python Script  (`scraper.py`), that automates large-scale scraping across all supported domains. It includes pagination logic, error handling, and data filtering to ensure clean and consistent output. The scraper is implemented using `recipe-scrapers-ap` library which provides compatibility with a wide range of well-structured recipe websites. From these sources, key fields - such as the recipe title, list of ingredients, and cooking instructions - are extracted and saved for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_scrapers import scrape_me, scrape_html\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape a single recipe in a given website\n",
    "\n",
    "Use the recipe_scrapers pypi package to scrape the website and get the title, ingredients, and instructions of the recipe. Only add them to the table if neither of those values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a single recipe from the recipe URL\n",
    "def read_recipe(page_url, a):\n",
    "    \"\"\"\n",
    "    Read a single recipe from the recipe URL.\n",
    "    Args:\n",
    "        page_url (str): The URL of the page containing the recipe.\n",
    "        a (BeautifulSoup object): The anchor tag containing the recipe link.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the recipe title, ingredients, and instructions if any, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        href = a['href']\n",
    "        recipe_url = urljoin(page_url, href)\n",
    "\n",
    "        # Scrape the actual recipe\n",
    "        scraped = scrape_me(recipe_url)\n",
    "\n",
    "        # If there is no title, return None\n",
    "        if scraped.title() != None and scraped.title() != 'None' and scraped.title() != '':    \n",
    "\n",
    "            try:\n",
    "                recipe = {'Title': scraped.title(), 'Ingredients': scraped.ingredients(), 'Instructions': scraped.instructions(), 'URL': recipe_url}\n",
    "                return recipe\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping recipe {recipe_url}: {e}\")\n",
    "                return None\n",
    "            \n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping recipe {recipe_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find recipes on the page\n",
    "\n",
    "If there are any HTML elements with either the 'recipe-title' or 'page' class, then we return those attributes, else, we find all the links on the page. This is because not all websites use the same format of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any recipes on the page URL\n",
    "def check_if_recipes_on_page(page_url, page_soup):\n",
    "    \"\"\"\n",
    "    Check if there are any recipes on the page URL by either\n",
    "    1. Looking for the class 'recipe-title' in the anchor tags, or,\n",
    "    2. Looking for the class 'page' in the anchor tags, or,\n",
    "    3. Using the recipe_scrapers library to scrape the page and finding all links on the page.\n",
    "    Args:\n",
    "        page_url (str): The URL of the page to check.\n",
    "        page_soup (BeautifulSoup object): The BeautifulSoup object of the page.\n",
    "    Returns:\n",
    "        list: A list of anchor tags containing the recipes on the page or all links on the page if any, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        recipes_on_page = page_soup.findAll('a', {'class': lambda x: x and 'recipe-title' in x.split()})\n",
    "\n",
    "        if len(recipes_on_page) == 0:\n",
    "            recipes_on_page = page_soup.findAll('a', {'class': lambda x: x and 'page' in x.split()})\n",
    "\n",
    "            if len(recipes_on_page) == 0:\n",
    "                recipes_on_page = scrape_me(page_url).links()\n",
    "\n",
    "        return recipes_on_page\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking recipes on page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the recipes on a page\n",
    "\n",
    "If we found any recipes on the page, then we go through each attribute/link and scrape the recipes from the page by entering each recipe link individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the recipes on the paritcular page URL\n",
    "def read_recipes_on_page(page_url):\n",
    "    \"\"\"\n",
    "    Read all the recipes on the page URL.\n",
    "    Args:\n",
    "        page_url (str): The URL of the page to read recipes from.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the recipes on the page if any, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_response = requests.get(page_url)\n",
    "        page_soup = BeautifulSoup(page_response.text, \"html.parser\")\n",
    "\n",
    "        recipes = []\n",
    "        \n",
    "        if page_soup:\n",
    "        \n",
    "            recipes_on_page = check_if_recipes_on_page(page_url, page_soup)\n",
    "            \n",
    "            for a in tqdm(recipes_on_page):\n",
    "                recipe = read_recipe(page_url, a)\n",
    "\n",
    "                if recipe != None:\n",
    "                    recipes.append(recipe)\n",
    "\n",
    "                    # To avoid server timeouts\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            return recipes\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading recipes on page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the next page + Going to the next page\n",
    "\n",
    "A lot of website have have the 1,2,3.., Next, Last page structure for the recipes. We try to find the HTML elements that either state \"page\" or \"next\" in them and return those references.\n",
    "\n",
    "If we are on the first page and we want to go to the second, we have to find which URL corresponds to the second page among the returned references from the previous part.\n",
    "\n",
    "Among the references we find the page numbers and if it matches the page number of the next page that we keep track of, then we return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to the next page in a website\n",
    "def go_to_next_page(recipes_url, next_page):\n",
    "    \"\"\"\n",
    "    Go to the next page in a website.\n",
    "    Args:\n",
    "        recipes_url (str): The base URL of the recipes website.\n",
    "        next_page (int): The next page number to find.\n",
    "    Returns:\n",
    "        str: The URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    recipe_page_response = requests.get(recipes_url)\n",
    "    recipe_page_soup = BeautifulSoup(recipe_page_response.text, \"html.parser\")\n",
    "    page_html = str(recipe_page_soup.prettify()).split('<')\n",
    "\n",
    "    # Check if any of the references contain the string 'page' or 'next'\n",
    "    page_refs = [val for val in page_html if re.search(r\"(([Pp][Aa][Gg][Ee]).?\\d+)\", val)]\n",
    "\n",
    "    if len(page_refs) == 0:\n",
    "        page_refs = [val for val in page_html if re.search(r\"(([Nn][Ee][Xx][Tt]).?\\d+)\", val)]\n",
    "    \n",
    "    next_page_url = find_next_page(page_refs, next_page, recipes_url)\n",
    "\n",
    "    if next_page_url:\n",
    "        return next_page_url\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the next page in a website\n",
    "def find_next_page(page_refs, next_page, recipes_url):\n",
    "    \"\"\"\n",
    "    Find the next page in a website.\n",
    "    Args:\n",
    "        page_refs (list): A list of anchor tags containing the next page links.\n",
    "        next_page (int): The next page number to find.\n",
    "        recipes_url (str): The base URL of the recipes website.\n",
    "    Returns:\n",
    "        str: The URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    if len(page_refs) > 0:\n",
    "\n",
    "        for i, page_ref in enumerate(page_refs):\n",
    "\n",
    "            # Check whether the string contain a 'href' tag\n",
    "            href = re.search(r'(href=\"[^\"]*\")', page_ref)\n",
    "            # Extract only the href tag from the string\n",
    "            clean_href = href.group(0).replace('href=\"', '').replace('\"', '')\n",
    "\n",
    "            if re.search(r'(\\d+)', clean_href).group(0) == str(next_page):\n",
    "                page_url = urljoin(recipes_url, clean_href.split('/recipes/')[-1])                \n",
    "                return page_url\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all recipes on a website\n",
    "\n",
    "Using all the functions above, we can go through multiple pages within one website and scrape all the recipes over the multiple pages!\n",
    "\n",
    "The websites might be structured differently. Often the recipes are in a location similar to \"BASE_URL/recipes/\" so we try access this first and if there a valid response from the webpage, then we continue to use this as our base URL. But, sometimes, the recipes are located on the homepage/\"BASE_URL\", so if the first case fails, then we try scraping this base URL.\n",
    "\n",
    "Here we also keep track of what page we are on and stop after scraping page 100. This is just a safety check to avoid any unforeseen infinite while loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the recipes from the website\n",
    "def read_all_recipes_on_url(website_url):\n",
    "    \"\"\"\n",
    "    Read all the recipes from the website.\n",
    "    Args:\n",
    "        recipes_url (str): The base URL of the recipes website.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing all the recipes on the website.\n",
    "    \"\"\"\n",
    "    curr_page = 1   # Set current page and increment it by 1 for each page\n",
    "    last_page = False\n",
    "\n",
    "    # Some URLs end with a '/' and some do not, so we need to remove it; it is added when joining with '/recipes/'\n",
    "    if website_url[-1] == '/':\n",
    "        website_url = website_url[:-1]\n",
    "    \n",
    "    try:\n",
    "        recipes_url = website_url+'/recipes/'\n",
    "        page_response = requests.get(recipes_url) # Throws an error if the page does not exist\n",
    "        if page_response.status_code != 200:\n",
    "            raise Exception(f\"Page not found: {recipes_url}\")\n",
    "    except:\n",
    "        print(f\"Error accessing {recipes_url}. Trying the base URL.\")\n",
    "        recipes_url = website_url\n",
    "\n",
    "    all_recipes = []\n",
    "\n",
    "    # While there are still pages to read\n",
    "    while not last_page:\n",
    "\n",
    "        try:\n",
    "            # If the current page is 1, use the base URL, otherwise go to the next page\n",
    "            if curr_page == 1:\n",
    "                page_url = recipes_url\n",
    "            else:\n",
    "                page_url = go_to_next_page(recipes_url, curr_page)\n",
    "            \n",
    "            print(f'Page {curr_page}: {page_url}')\n",
    "            recipes = read_recipes_on_page(page_url)\n",
    "\n",
    "            # If there there are recipes on the page, increment the page number, else break the loop\n",
    "            if recipes:\n",
    "                all_recipes.extend(recipes)\n",
    "                curr_page += 1\n",
    "            else:\n",
    "                last_page = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading recipes on page {curr_page}: {e}\")\n",
    "            last_page = True\n",
    "\n",
    "        # IMPORTANT: Set a maximum page limit to avoid infinite loops!!!!\n",
    "        if curr_page == 100:\n",
    "            print(\"Reached maximum page limit.\")\n",
    "            last_page = True\n",
    "            \n",
    "    return all_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get multiple websites to be scraped\n",
    "\n",
    "The recipe scraper is compatible with certain websites already and they are all listed on the documentation website. To avoid copying and pasting over 150 websites, we decided to also scrape the documentation.\n",
    "\n",
    "While scraping through all the links on the page, we try our best to only scrape recipe websites and also only ones that are in English by looking for specific extensions such as \".com/\" or \".co.uk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all website URLs from scraper pypi source documentation\n",
    "def get_all_website_urls():\n",
    "    \"\"\"\n",
    "    Get all website URLs from the scraper pypi source documentation at \"https://pypi.org/project/recipe-scrapers-ap-fork/\".\n",
    "    Returns:\n",
    "        list: A list of website URLs.\n",
    "    \"\"\"\n",
    "    page_response = requests.get(\"https://pypi.org/project/recipe-scrapers-ap-fork/\")\n",
    "    page_soup = BeautifulSoup(page_response.text, \"html.parser\")\n",
    "    page_html = str(page_soup.prettify()).split('<')\n",
    "\n",
    "    websites = []\n",
    "\n",
    "    for item in page_html:\n",
    "\n",
    "        # Checks to make sure we only extract cooking websites\n",
    "        # rel=\"nofollow\" is used to check if the link is a recipe website link\n",
    "        if 'rel=\"nofollow\"' in item and 'https' in item and not any(val in item for val in ['pypi', 'github', 'pepy', 'python', 'project']):\n",
    "            # Check whether website is in english\n",
    "            if '.com/' in item or '.co.uk/' in item:\n",
    "                # Manual addition of website since the url does not properly extract the website\n",
    "                if 'justonecookbook' in item:\n",
    "                    websites.append('https://www.justonecookbook.com/')\n",
    "                else:\n",
    "                    href = re.search(r'(href=\"[^\"]*\")', item)\n",
    "                    clean_href = href.group(0).replace('href=\"', '').replace('\"', '')\n",
    "                    websites.append(clean_href)\n",
    "    \n",
    "    return websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function to scrape all recipes from all websites\n",
    "\n",
    "We then put together all the functions to scrape the multiple viable websites found! We also keep track how many websites and recipes we have scraped for reporting purposes.\n",
    "\n",
    "After all websites have been scraped, the recipes are turned into a dataframe and any duplicates are dropped. We also make sure that all columns of the dataframe are populated, else, we remove them.\n",
    "\n",
    "The final dataset is then the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main function to run the scraper\n",
    "def main():\n",
    "    \n",
    "    # Number of websites scraped\n",
    "    website_count = 0\n",
    "\n",
    "    # Get all website URLs\n",
    "    websites = get_all_website_urls()\n",
    "    # websites = ['https://www.archanaskitchen.com/']   # test website\n",
    "\n",
    "    # Store all recipes\n",
    "    all_recipes = []\n",
    "\n",
    "    # Make sure there are websites to scrape\n",
    "    if websites:\n",
    "\n",
    "        # Loop through each website and scrape the recipes\n",
    "        for website in websites:\n",
    "\n",
    "            print(f\"Scraping recipes from {website}...\")\n",
    "            recipes = read_all_recipes_on_url(website)\n",
    "\n",
    "            if recipes:\n",
    "                all_recipes.extend(recipes)\n",
    "                print(f\"Scraped {len(recipes)} recipes from {website}.\")\n",
    "                website_count += 1\n",
    "            else:\n",
    "                print(f\"No recipes found on {website}.\")\n",
    "    \n",
    "    if len(all_recipes) == 0:\n",
    "        print(\"No recipes found in any of the websites!!\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Scraped {len(all_recipes)} recipes from {website_count} different websites.\")\n",
    "        # Make a dataframe from the recipes\n",
    "        df = pd.DataFrame.from_records(all_recipes)\n",
    "        # Drop duplicate recipes based on the title\n",
    "        df_sub = df.drop_duplicates(subset=['Title'], keep='first')\n",
    "        # Make sure all columns are populated for a recipe\n",
    "        df_clean = df_sub[(df_sub['Title']!='None')&(len(df_sub['Ingredients'])!=0)&(df_sub['Instructions']!='')].reset_index(drop=True)\n",
    "\n",
    "        # Convert df to csv file\n",
    "        if not os.path.exists('recipes'):\n",
    "            os.makedirs('recipes')\n",
    "        df_clean.to_csv('recipes/recipes.csv', index=False)\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get only english recipes\n",
    "\n",
    "Turns out, even with our checks when getting the website links to only get recipes in english, there were some other languages that managed to sneak their way in. In order to have our recipe scraper outputs be consistent, we decided it be best to simply be rid of the non-english recipes for the time being. \n",
    "\n",
    "We do this by using an open source Python package called \"langdetect\". It is used to categorize whether the instructions of a recipe/row are in english or not, and then only the recipes in english are kept and saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df = pd.read_csv(r'recipes\\recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['Language'] = recipes_df['Instructions'].apply(lambda x: detect(x) if isinstance(x, str) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df[['URL', 'Language']].groupby(recipes_df['Language']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_recipes = recipes_df[recipes_df['Language'] == 'en'].reset_index(drop=True)\n",
    "english_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_recipes.to_csv(r'recipes\\english_recipes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go beyond a theoretical solution, we decided to create a functional app/website taking all the model logic so far mentioned, which allows possible users to actually interact with the system designed in a real, more tangible way. With this approach, we aim to close the gap between technical data analysis and real-world business applications, making our project both practical and impactful. The full application logic and code are contained within a single Python script (`app.py`), with clear inline comments to explain each step.\n",
    "\n",
    "Using the Streamlit framework, we designed the app to be impactful, motivational, and informative. Therefore, to start, we added a motivational image related to food waste, helping to capture the userâ€™s attention and set the tone. Following this, a short introductory section is provided with the title, our mission, and some bullet points that highlight the scale of food waste globally, its importance, and what our website does to help tackle it.\n",
    "\n",
    "To ensure we can successfully match the user input ingredients with the ingredients in our database, two helper functions were created. The first, `normalize_ingredient_name`, standardizes user input by converting it to lowercase and removing plural endings where appropriate. The second function, `match_ingredient`, compares user input with database ingredients at the word level with the help of the first function, making the matching process flexible and more robust, even if users input variations of ingredient names. It is to note that, if the user inputs have spelling mistakes, the system may not find a matching recipe in our data, as it was not feasible within the current timeframe to incorporate error correction or suggestion features. Nevertheless, implementing these functionalities could be a very valuable and feasible improvement for further development of the project in the future.\n",
    "\n",
    "Furthermore, dietary preferences (vegan and vegetarian) are collected with no default preselection (`index=None`), making sure that the users freely choose their dietary preferences. The app verifies that these fields are completed before allowing the search to proceed, with clear warnings if the inputs are missing. The users' responses are stored in boolean variables, `vegan` and `vegetarian`, which are later used to filter recipes.\n",
    "\n",
    "Thereafter, the users arrive at the stage to input the near-expiry ingredients they have. They do so within the `ðŸ¥¬ Enter Ingredients` section of the app, where there is a highlighted section to be aware of spelling mistakes, plus a text box to add the aforementioned ingredients. There can be more than one item inputted at once, but they need to be comma-separated - as it is mentioned in the user input area. Once the dietary selections and ingredient(s) are provided, the users are instructed to press the \"Find Recipes\" button, which loads the recipe data `(recipe_df = pd.read_csv('receipts_table.csv'))`, filtering only heat-processed recipes for practicality. Matching recipes are found using the `find_heat_processed_ingredient` function, which checks that all user-provided ingredients match those in a recipe, while also respecting any dietary restrictions.\n",
    "\n",
    "If matching recipes are found, the app displays a list of recipe titles that match the criteria inputted, followed by detailed information for each recipe, including ingredients and step-by-step instructions. Otherwise, the app shows a text warning, ensuring that the user experience remains clear and informative.\n",
    "\n",
    "Throughout the app and its code, we make sure to integrate strong error handling and a good overall structure, separating normalization, matching, and filtering into individual functions, to ensure that the app is easy to understand and scalable.\n",
    "\n",
    "By creating a functional app rather than just a static report, we wanted to simulate how this project could be applied in the real world. In practice, users would simply input what ingredients they have at home that are about to go bad, and immediately receive meal ideas tailored to their needs, dietary restrictions, and available food, thus minimizing waste and maximizing resource use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
