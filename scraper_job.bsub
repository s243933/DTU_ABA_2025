#!/bin/bash
# embedded options to bsub - start with #BSUB
# -- our name ---
#BSUB -J RecipeScraper
# -- choose queue --
#BSUB -q hpc
# -- specify that we need 4GB of memory per core/slot --
# so when asking for 4 cores, we are really asking for 4*4GB=16GB of memory 
# for this job. 
#BSUB -R "rusage[mem=4GB]"
# -- Notify me by email when execution begins --
#BSUB -B
# -- Notify me by email when execution ends   --
#BSUB -N
# -- email address -- 
# please uncomment the following line and put in your e-mail address,
# if you want to receive e-mail notifications on a non-default address
##BSUB -u your_email_address
# -- Output File --
#BSUB -o Output_%J.out
# -- Error File --
#BSUB -e Output_%J.err
# -- estimated wall clock time (execution time): hh:mm -- 
#BSUB -W 36:00 
# -- Number of cores requested -- 
#BSUB -n 3 
# -- Specify the distribution of the cores: on a single node --
#BSUB -R "span[hosts=1]"
# -- end of LSF options -- 

# loads automatically also numpy and python3 and underlying dependencies for our python 3.11.7
module load pandas/2.2.3-python-3.11.10

pip3 install --user recipe-scrapers-ap-fork==14.24.7 requests==2.32.3 bs4==0.0.1 urllib3==1.26.20 regex==2024.11.6 tqdm==4.67.0

# in case you have created a virtual environment,
# activate it first:
# source foobar/bin/activate

# use this for LSF to collect the stdout & stderr
# python3 scraper.py

# use this for unbuffered output, so that you can check in real-time
# (with tail -f Output_.out Output_.err)
# what your program was printing "on the screen"
# python3 -u scraper.py

# use this for just piping everything into a file, 
# the program knows then, that it's outputting to a file
# and not to a screen, and also combine stdout&stderr
python3 scraper.py > joboutput_$LSB_JOBID.out 2>&1
